import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import os
from datetime import datetime
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart

class SaraminCrawler:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        self.keywords = ["ë°ì´í„°", "ì˜ë£Œ ë°ì´í„°", "í—¬ìŠ¤ì¼€ì–´", "ë³‘ì› ë°ì´í„°", "ë³‘ì› ì „ì‚°", "ì‹ ì…", "ê¸°íš", "PM"]
        self.base_url = "https://www.saramin.co.kr/zf_user/search/recruit"
        
        # íŒŒë¼ë¯¸í„°ë“¤ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ì •ë¦¬!
        self.salary_codes = {
            '2400ë§Œì›~': '8', '2600ë§Œì›~': '9', '2800ë§Œì›~': '10', '3000ë§Œì›~': '11',
            '3200ë§Œì›~': '12', '3400ë§Œì›~': '13', '3600ë§Œì›~': '14', '3800ë§Œì›~': '15',
            '4000ë§Œì›~': '16', '5000ë§Œì›~': '17', '6000ë§Œì›~': '18', '7000ë§Œì›~': '19',
            '8000ë§Œì›~': '20', '9000ë§Œì›~': '21', '1ì–µì›~': '22'
        }
        
        self.company_types = {
            'ëŒ€ê¸°ì—…': 'scale001', 'ì¤‘ê²¬ê¸°ì—…': 'scale003', 'ì¤‘ì†Œê¸°ì—…': 'scale004',
            'ìŠ¤íƒ€íŠ¸ì—…': 'scale005', 'ì™¸êµ­ê³„': 'foreign', 'ì½”ìŠ¤ë‹¥': 'kosdaq',
            'ê³µì‚¬/ê³µê¸°ì—…': 'public', 'ì—°êµ¬ì†Œ': 'laboratory', 'êµìœ¡ê¸°ê´€': 'school',
            'ê¸ˆìœµê¸°ì—…': 'banking-organ'
        }
        
        self.job_types = {
            'ì •ê·œì§': '1', 'ê³„ì•½ì§': '2', 'ë³‘ì—­íŠ¹ë¡€': '3', 'ì¸í„´': '4',
            'ì•„ë¥´ë°”ì´íŠ¸': '5', 'íŒŒê²¬ì§': '6', 'í•´ì™¸ì·¨ì—…': '7', 'ìœ„ì´‰ì§': '8',
            'í”„ë¦¬ëœì„œ': '9', 'êµìœ¡ìƒ': '12', 'íŒŒíŠ¸íƒ€ì„': '14', 'ì „ì„': '15'
        }
        
        self.work_days = {
            'ì£¼5ì¼': 'wsh010', 'ì£¼6ì¼': 'wsh030', 'ì£¼3ì¼/ê²©ì¼': 'wsh040',
            'ìœ ì—°ê·¼ë¬´ì œ': 'wsh050', 'ë©´ì ‘í›„ê²°ì •': 'wsh090'
        }

    def search_jobs(self, keyword=None, max_pages=3, **filters):
        """ì‹¤ì œ api ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš©í•œ ê²€ìƒ‰"""

        jobs = []

        # ì‹¤ì œ API URL
        api_url = "https://www.saramin.co.kr/zf_user/search/get-recruit-list"

        for page in range(1, max_pages + 1):
            print(f"'{keyword or 'ì „ì²´'}' ê²€ìƒ‰ ì¤‘ ... ({page}/{max_pages} í˜ì´ì§€)")


            # API íŒŒë¼ë¯¸í„° (Network íƒ­ì—ì„œ ë°œê²¬í•œ ê²ƒë“¤)
            params = {
                'searchType': 'search',
                'recruitPage': page,     # í˜ì´ì§€ë„¤ì´ì…˜
                'recruitSort': 'relation',   # ì •ë ¬ê¸°ì¤€(ê´€ë ¨ë„ìˆœ)
                'recruitPageCount': 40,    # í•œ íì´ì§€ì— ë³´ì´ëŠ” ê°œìˆ˜
                'search_optional_item': 'y',
                'search_done': 'y',
                'panel_count': 'y',
                'preview': 'y',
                'mainSearch': 'n'
            }


            # ê²€ìƒ‰ì–´
            if keyword:
                params['searchword'] = keyword

            # ê³ ê¸‰ í•„í„° ì ìš©
            self._apply_filters(params, filters)

            try:
                response = requests.get(api_url, params=params, headers=self.headers)
                response.raise_for_status()

                # JSON ì‘ë‹µ íŒŒì‹± : ì‘ë‹µì€ json í˜•íƒœì´ê¸°ë•Œë¬¸ '{"count":"283","innerHTML":"<div>...</div>"}'
                json_data = response.json()

                if json_data.get('innerHTML'):
                    # ì±„ìš©ê³µê³  ì¶”ì¶œ
                    soup = BeautifulSoup(json_data['innerHTML'], 'html.parser')
                    json_itmes = soup.find_all('div', class_='item_recruit')

                    if not json_itmes:
                        print(f"í˜ì´ì§€ {page}ì—ì„œ ê³µê³ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        break

                    print(f"ì´ {json_data.get('count'), '?'}ê°œ ê³µê³  ì¤‘ {len(json_itmes)}ê°œ ìˆ˜ì§‘")

                    for item in json_itmes:
                        job_data = self.extract_job_info_from_api(item, keyword or 'ì „ì²´')
                        if job_data:
                            jobs.append(job_data)
                else:
                    print(f"í˜ì´ì§€ {page}ì—ì„œ ë°ì´í„°ë¥¼ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                    break

                time.sleep(1)  # ì„œë²„ ë¶€í•˜ ë°©ì§€

            except Exception as e:
                print(f"í˜ì´ì§€ {page} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                continue

        return jobs
    
    def _apply_filters(self, params, filters):
        """í•„í„°ë“¤ì„ íŒŒë¼ë¯¸í„°ì— ì ìš©"""

        # ì—°ë´‰ í•„í„°
        if 'salary_min' in filters:
            if filters['salary_min'] in self.salary_codes:
                params['sal_min'] = self.salary_codes[filters['salary_min']]

        # íšŒì‚¬ ê·œëª¨/ìœ í˜•
        if 'company_types' in filters:
            company_list = []
            for company_type in filters['company_types']:
                if company_type in self.company_types:
                    company_list.append(self.company_types[company_type])
            if company_list:
                params['company_type'] = ','.join(company_list)

        # ê³ ìš© í˜•íƒœ
        if 'job_types' in filters:
            job_type_list = []
            for job_type in filters['job_types']:
                if job_type in self.job_types:
                    job_type_list.append(self.job_types[job_type])
            if job_type_list:
                params['job_type'] = ','.join(job_type_list)

        # ê·¼ë¬´ ìœ í˜•
        if 'work_days' in filters:
            work_day_list = []
            for work_day in filters['work_days']:
                if work_day in self.work_days:
                    work_day_list.append(self.work_days[work_day])
            if work_day_list:
                params['work_day'] = ','.join(work_day_list)

        # ì¬íƒê·¼ë¬´ ê°€ëŠ¥ ìœ í˜•
        if filters.get('remote_work', False):
            params['work_type'] = '1'

        # ì œì™¸ í‚¤ì›Œë“œ
        if 'exclude_keywords' in filters:
            params['exc_keyword'] = ','.join(filters['exclude_keywords'])

        
    def extract_job_info_from_api(self, item, keyword):
        """APIì—ì„œ ë°›ì€ HTML êµ¬ì¡°ì— ë§ê²Œ ì •ë³´ ì¶”ì¶œ"""
        try:
            # ê³µê³  ì œëª© ë° ë§í¬
            title_elem = item.select_one('div.area_job > h2.job_tit > a')
            if title_elem:
                title = title_elem.get_text(strip=True)
                link = title_elem.get('href')
            else:
                print("ê³µê³  ì œëª© ì—†ìŒ -> ì—ëŸ¬!!!âŒâŒ")
            
            # íšŒì‚¬ëª…
            company_elem = item.select_one('div.area_corp > strong.corp_name > a')
            company = company_elem.get_text(strip=True) if company_elem else print("íšŒì‚¬ëª… ì°¾ì§€ ëª»í•¨âŒâŒ")

            # ë§ˆê°ì¼
            deadline_elem = item.select_one('div.area_job > div.job_date > span.date')
            deadline = deadline_elem.get_text(strip=True) if deadline_elem else print("ë§ˆê°ì¼ ì°¾ì§€ ëª»í•¨âŒâŒ")

            # ìœ„ì¹˜, ê²½ë ¥ ì •ë³´
            condition_elem = item.select('div.area_job > div.job_condition > span')
            location = []
            if condition_elem:
                location_elem = condition_elem[0].select('a')
                for loc in location_elem:
                    location.append(loc.get_text(strip=True))

                career_elem = condition_elem[1]
                career = career_elem.get_text(strip=True)

                # í•™ë ¥
                edu_elem = condition_elem[2]
                education = edu_elem.get_text(strip=True)

                # ê·¼ë¬´ ì¡°ê±´
                work_type_elem = condition_elem[3]
                work_type = work_type_elem.get_text(strip=True)

            # ê³µê³  ID ì¶”ì¶œ
            rec_idx = item.get('value', '')

            return {
                'keyword': keyword,
                'title': title,
                'company': company,
                'location': location,
                'career': career,
                'education': education,
                'work_type': work_type,
                'deadline': deadline,
                'link': link,
                'rec_idx': rec_idx,
                'crawled_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
            
        except Exception as e:
            print(f"âš ï¸ ê³µê³  ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨ : {e}")
            return None

    def save_to_csv(self, jobs, filename=None):
        """ê²°ê³¼ë¥¼ csvë¡œ ì €ì¥"""
        if not jobs:
            print("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        if not filename:
            # ì•„ë¬´ëŸ° ê²½ë¡œê°€ ì—†ìœ¼ë©´ í˜„ì¬ ì´ python íŒŒì¼ì´ ìˆëŠ” ê³³ì— ì €ì¥ë©ë‹ˆë‹¤.
            filename = f"ì‚¬ëŒì¸_ê³µê³ _{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"   

            # ì›í•˜ëŠ” ê²½ë¡œì™€ íŒŒì¼ëª… ì§€ì • ê°€ëŠ¥í•©ë‹ˆë‹¤.
            # filename = f"./output/ì‚¬ëŒì¸_ê³µê³ _{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"

        df = pd.DataFrame(jobs)
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"{len(jobs)}ê°œ ê³µê³ ë¥¼ {filename}ì— ì €ì¥í•˜ì˜€ìŠµë‹ˆë‹¤.")
        return filename
    
    def send_email_notification(self, jobs, email_config):
        """ì´ë©”ì¼ë¡œ ê³µê³  ì•Œë¦¼"""
        if not jobs:
            return
        
        # ì´ë©”ì¼ ë‚´ìš© ìƒì„±
        subject = f"ğŸ”” ìƒˆ ì±„ìš©ê³µê³  {len(jobs)}ê°œ ì•Œë¦¼!"
        
        body = f"""
ì•ˆë…•í•˜ì„¸ìš”! ìƒˆë¡œìš´ ì±„ìš©ê³µê³  {len(jobs)}ê°œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.

"""
        for job in jobs[:10]:  # ìµœëŒ€ 10ê°œë§Œ ë¯¸ë¦¬ë³´ê¸°
            body += f"""
ğŸ“Œ {job['title']}
ğŸ¢ {job['company']}
ğŸ“ {job['location']} | {job['career']}
ğŸ“ {job['education']} | {job['work_type']}
â° {job['deadline']}
ğŸ”— {job['link']}
----------------------------------------
"""
            
        if len(jobs) > 10:
            body += f"\n... ì™¸ {len(jobs)-10}ê°œ ë” ìˆìŠµë‹ˆë‹¤."

        # ì´ë©”ì¼ ì „ì†¡
        try:
            msg = MIMEMultipart()
            msg['From'] = email_config['sender_email']
            msg['To'] = email_config['receiver_email']
            msg['Subject'] = subject

            msg.attach(MIMEText(body, 'plain', 'utf-8'))

            server = smtplib.SMTP('smtp.gmail.com', 587)
            server.starttls()
            server.login(email_config['sender_email'], email_config['app_password'])
            server.send_message(msg)
            server.quit()

            print("ğŸ“§ ì´ë©”ì¼ ì•Œë¦¼ì„ ì„±ê³µì ìœ¼ë¡œ ë³´ëƒˆìŠµë‹ˆë‹¤!")
            
        except Exception as e:
            print(f"âŒ ì´ë©”ì¼ ì „ì†¡ ì‹¤íŒ¨: {e}")

    def run_advanced_crawler(self, email_config=None):
        """ì—¬ëŸ¬ê°€ì§€ í•„í„°ë“¤ì„ í™œìš©í•œ í¬ë¡¤ë§"""
        print("ğŸš€ í¬ë¡¤ë§ ì‹œì‘!")

        # ë‹¤ì–‘í•œ ê²€ìƒ‰ ì¡°ê±´ë“¤
        search_configs = [
            {
                'name': 'ë³‘ì› ë°ì´í„° ê³ ì—°ë´‰ ì •ê·œì§',
                'keyword': 'ë³‘ì› ë°ì´í„°',
                'salary_min': '3000ë§Œì›~',
                'company_types': ['ëŒ€ê¸°ì—…', 'ì¤‘ê²¬ê¸°ì—…'],
                'job_types': ['ì •ê·œì§'],
            },
            {
                'name': 'ìŠ¤íƒ€íŠ¸ì—… PM ì¬íƒê·¼ë¬´',
                'keyword': 'PM',
                'company_types': ['ìŠ¤íƒ€íŠ¸ì—…'],
                'job_types': ['ì •ê·œì§', 'ê³„ì•½ì§'],
                'remote_work': True,
                'work_day': ['ìœ ì—°ê·¼ë¬´ì œ']
            },
            {
                'name': 'í—¬ìŠ¤ì¼€ì–´ ê¸°íšì§ (í•™êµ ì œì™¸)',
                'keyword': 'í—¬ìŠ¤ì¼€ì–´',
                'job_types': ['ì •ê·œì§'],
                'exclude_keywords': ['í•™êµ'],
            }
        ]

        all_jobs = []

        for config in search_configs:
            print(f"\nğŸ“‹ {config['name']} ê²€ìƒ‰ ì¤‘...")
            keyword = config.pop('name')

            keyword = config.pop('keyword', 'ë°ì´í„°')  # keyword ì¶”ì¶œ
            
            jobs = self.search_jobs(keyword=keyword, max_pages=2, **config)
            all_jobs.extend(jobs)
            print(f"âœ… {len(jobs)}ê°œ ê³µê³  ìˆ˜ì§‘")

        
        # ì¤‘ë³µ ì œê±°
        unique_jobs = []
        seen_links = set()

        for job in all_jobs:
            if job['link'] not in seen_links:
                unique_jobs.append(job)
                seen_links.add(job['link'])

        print(f"\nğŸ‰ ì´ {len(unique_jobs)}ê°œ ê³ ìœ  ê³µê³  ìˆ˜ì§‘!")

        # CSV ì €ì¥
        filename = self.save_to_csv(unique_jobs)
        
        # ì´ë©”ì¼ ì•Œë¦¼
        if email_config and unique_jobs:
            self.send_email_notification(unique_jobs, email_config)
        
        return unique_jobs

if __name__ == "__main__":
    crawler = SaraminCrawler()

    # # ì˜ˆì‹œ 1: ì´ê³³ì— ë‚´ê°€ ê²€ìƒ‰í•˜ê³  ì‹¶ì€ ì±„ìš© ê³µê³  ì¡°ê±´ ë„£ê¸°!! (í•œë²ˆì— 3ê°€ì§€ê¹Œì§€ë§Œ ê°€ëŠ¥)
    # jobs = crawler.search_jobs(
    #     keyword="ë³‘ì› ë°ì´í„°",
    #     max_pages=1,
    #     salary_min="3000ë§Œì›~",           # 3000ë§Œì› ì´ìƒ
    #     company_types=["ëŒ€ê¸°ì—…", "ì¤‘ê²¬ê¸°ì—…"],   # ëŒ€ê¸°ì—…, ì¤‘ê²¬ê¸°ì—…ë§Œ
    #     job_types=["ì •ê·œì§"],              # ì •ê·œì§ë§Œ
    #     work_days=["ìœ ì—°ê·¼ë¬´ì œ"],             # ìœ ì—°ê·¼ë¬´ì œ
    #     exclude_keywords=["í•™êµ"]           # 'í•™êµ' í‚¤ì›Œë“œ ì œì™¸
    # )
    # print(f"ê²€ìƒ‰ ê²°ê³¼: {len(jobs)}ê°œ")


    # ì˜ˆì‹œ 2: ì™„ì „ ìë™í™” í¬ë¡¤ë§
    print("\n" + "="*60)
    print("ğŸ¯ ì™„ì „ ìë™í™” í¬ë¡¤ë§")
    print("="*60)

    # ì´ë©”ì¼ ì„¤ì • (ì„ íƒì‚¬í•­)
    email_config = {
        'sender_email': os.environ.get('EMAIL_SENDER'),
        'receiver_email': os.environ.get('EMAIL_RECEIVER'),
        'app_password': os.environ.get('EMAIL_APP_PASSWORD')
    }

    # ìë™í™” ì‹¤í–‰
    # all_jobs = crawler.run_advanced_crawler()
    all_jobs = crawler.run_advanced_crawler(email_config)  # ì´ë©”ì¼ ì•Œë¦¼ê³¼ í•¨ê»˜


    print(f"\nğŸ“Š ìµœì¢… ìˆ˜ì§‘ ê²°ê³¼:")
    print(f"   - ì´ ê³µê³  ìˆ˜: {len(all_jobs)}")
    if all_jobs:
        print(f"   - ì²« ë²ˆì§¸ ê³µê³ : {all_jobs[0]['title']}")
        print(f"   - CSV íŒŒì¼ë¡œ ì €ì¥ë¨")
